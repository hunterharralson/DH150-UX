# DH150 Assignment 02 - Usability Testing Pilot

## Introduction
<a href="http://letterboxd.com" target="_blank"> Letterboxd </a> is a website that serves as a social platform for cinephiles, Letterboxd allows a user to log viewed films, leave lengthy film reviews, rate movies, and discuss movies with friends and strangers. It serves as a medium for people to interact about the movies they love, hate, or simply want to share with their following. 
<p>&nbsp;</p>
Usability Testing is a method to evaluate how a given website or application is interacted with by its users. In testing how a user completes routine tasks, the user-centered design can be greatly improved. A usability test is conducted with a group of potential users in three typical settings: a usability lab, remotely, or on-site using portable equipment. These sessions are recorded, with their data analyzed to help identify potential improvements to the website or application. 
<p>&nbsp;</p>
I conducted a usability test of the website Letterboxd, using a live version of the website while testing my roommate remotely. With my roommate using ActivePresenter software to both screen record and face record, along with us connecting via FaceTime for me to give him instructions, the usability test was conducted. The test was designed to capture the participant's navigational choices, comments, satisfaction ratings, and general feedback; however, some of these facets were lost due to technical diffulties with the software. Luckily, as administrator/moderator, I was very attentive and taking notes of the test and responses. 

---

## Methodology
For me, recruiting a participant for the pilot study was not a difficult process. My roommate is very much a cinephile, and he is the one who introduced Letterboxd to me a while ago. After asking if he would like to participate in this test, he agreed, and we set up a time that worked for both of us. Letterboxd is not a website that everyone would use, so I wanted to get a feel for how the actual user of the website interacts with it. After making sure the software was all set to go, we began our remote test, which I moderated as he completed the tasks. 
<p>&nbsp;</p>
During the session, the test administrator (me) explained the test session to the participant and asked the participant to fill out a background questionnaire about prior experience with Letterboxd. The administrator read the tasks to the participant and had him attempt to complete them on the website. 
<p>&nbsp;</p>
Following the completion of the three tasks, the participant filled out a questionnaire regarding the tasks. The participant responded on a 7 point Likert scale to these questions:
* How easy or difficult was it to perform the tasks? (very easy - very difficult)
* How much time do you think it took to complete the tasks? (much shorter than expected - much longer than expected)
* How likely are you to do this task? (very unlikely - very likely)
<p>&nbsp;</p>
The participant was then asked how much they agreed with the following statements, answering on a 7 point Likert scale (strongly disagree to strongly agree):
* I think that I would like to use this website frequently
* I found the website unnecessarily complex
* I thought the website was easy to use
* I think that I would need the support of a technical person to be able to use this website
* I found the various functions in this website were well integrated
* I thought there was too much inconsistency in this website
* I would imagine that most people would learn to use this website very quickly
* I found the website very cumbersome to use
* I felt very confident using the website
* I needed to learn a lot of things before I could get going with this website
<p>&nbsp;</p>
Lastly, the participant was asked to select five words that he felt were most closely associated with Letterboxd. The participant was also asked to fill out a brief background survey sharing demographic information. 
<p>&nbsp;</p>
## Survey Link (usability test materials)
[here](https://forms.gle/mQTCnjqMyysgyLeh8)
<p>&nbsp;</p>
## Pilot Test Video:
[here](https://drive.google.com/file/d/1LeC3ka1fxsSPTg_XFFHxZoz-9c3dJZbs/view?usp=sharing)
**Important note: In the process of exporting the pilot video from ActivePresenter into a .mp4 format, the participant's computer crashed. Since I could not conduct the interview in person due to Coronavirus issues, I had to have the participant download the software and attempt this himself. Luckily, before exporting, he had accidentally saved the face video of himself into an MKV format. However, this video has zero audio nor does it show the screen. I only attached it as proof that I conducted the interview, but don't have all the desired materials. However, I was watching his interaction via FaceTime and remember most of the user decisions and his explanations.**

--- 

## Reflection
Reflecting on the Usability Test, I feel that besides the major technical difficulty, everything went smoothly. The main thing I learned is that I need to take the technical matters in my hands more to reduce the chance of something bad happening. I also learned that as the moderator, I need to try to remain neutral and not interject my opinions whatsover, as that may bias the test. This part was difficult because it was harder to be formal with a close friend; however, I feel that this will improve with practice. 
In terms of future improvements, I think that providing positive feedback regardless of the response will be very helpful, as well as remaining clear in my directions as moderator. It is difficult to conduct these tests over FaceTime because there are already so many moving parts, but in a real setting not hindered by social distancing, I think this process will become easier. 
